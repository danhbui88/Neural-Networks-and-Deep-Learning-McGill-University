##Fashion MNIST Autoencoder Directives Create the autoencoders described below, you can play with the topology, those are just starting points

-Use GPU runtime

-Print 10 inputs and their associated outputs

-Don't forget to normalize your data

-Use the Functional API for Keras

-Plot the model loss over time

Autoencoders to build

-Stacked

-Convolutional Encoder

__________________________________________________
import numpy as np
import matplotlib.pyplot as plt
import time
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from keras.layers import InputLayer
from tensorflow.keras.layers import Dense,BatchNormalization
from keras.layers import Conv2D, UpSampling2D
from tensorflow.keras.optimizers import Adam
from keras.layers.pooling import MaxPooling2D

#Getting the data fashion_mnist from keras
fashion_mnist = keras.datasets.fashion_mnist
(x_train, _), (x_test, _) = fashion_mnist.load_data()

all_data = np.concatenate((x_train,x_test))
all_data.shape

#Print out first 10 images
def print_first_10(data):
  fig = plt.figure()
  for i in range(10):
    plt.subplot(4,4,i+1)
    plt.tight_layout()
    plt.imshow(data[i], interpolation='none')
    plt.title("Digit: {}".format(i))
    plt.xticks([])
    plt.yticks([])
print_first_10(all_data[-10:])

#Normalize data
X_train = x_train.reshape(60000, 784)
X_test = x_test.reshape(10000, 784)
X_train = x_train.astype("float64")
X_test = x_test.astype("float64")
all_data=all_data.reshape(70000,784)
all_data=all_data.astype('float64')
x_train=X_train/255
x_test=X_test/255
all_data=all_data/255

##Stacked AutoEncoder
model=Sequential()
model.add(Dense(784, activation='relu', input_shape=(784,)))
model.add(Dense(392, activation='relu', name='compressed'))
model.add(Dense(196, activation='relu'))
model.add(Dense(392, activation='relu'))
model.add(Dense(784, activation='relu'))
model.summary()
model.compile('adam',loss='mse')
early_stopping_monitor=EarlyStopping(monitor='val_loss',patience=5)
h=model.fit(all_data,all_data,epochs=20,validation_split=0.2,batch_size=128,callbacks=[early_stopping_monitor])

out_put_reg=model.predict(all_data.reshape(70000,784))
out_put=out_put_reg.reshape(70000,28,28)

#Print first 10 outputs
print_first_10(out_put[-10:])

##Try with CNN
tf=tf._api.v1.keras.layers
model1=Sequential()
model1.add(tf.Conv2D(8, (3,3), padding='same', strides=(1,1), activation='relu',input_shape=(28,28,1))) 
model1.add(tf.MaxPooling2D(pool_size=(2,2))) #[None,28,28,8]-[None,14,14,8]#

model1.add(tf.Conv2D(8, (3,3), padding='same', strides=(1,1), activation='relu'))
model1.add(tf.MaxPooling2D(pool_size=(2,2))) #[None,14,14,8]-[None,7,7,4]#

model1.add(tf.Conv2D(4, (3,3), padding='same', strides=(1,1), activation='relu')) #4 here is the number of filter
#model1.add(tf.MaxPooling2D(pool_size=(2,2))) #(None,7,7,4)#


#Decoder
model1.add(tf.Conv2D(4, (3,3), padding='same', strides=(1,1), activation='relu'))
model1.add(tf.UpSampling2D(size=(2, 2)))

model1.add(tf.Conv2D(4, (3,3), padding='same', strides=(1,1), activation='relu'))
model1.add(tf.UpSampling2D(size=(2, 2)))

model1.add(tf.Conv2D(8, (3,3), padding='same', strides=(1,1), activation='relu'))
model1.add(tf.UpSampling2D(size=(1, 1)))

model1.add(tf.Conv2D(1, (3,3), padding='same', strides=(1,1), activation='relu'))

model1.compile('adam','mse')
model1.summary()

all_data=all_data.reshape(-1,28,28,1)
h2=model1.fit(all_data,all_data,epochs=20,validation_split=0.2,batch_size=128,callbacks=[early_stopping_monitor])

#Plot Model Loss
from matplotlib.pyplot import figure
figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')
plt.plot(h2.history['loss'],label='CNN_Train_Loss')
plt.plot(h2.history['val_loss'],label='CNN_Val_Loss')
plt.plot(h.history['loss'],label='Stacked_Train_Loss')
plt.plot(h.history['val_loss'],label='Stacked_Val_Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='best')
plt.show()

#Print out first 10 outputs
out_put_reg=model1.predict(all_data)
out_put=out_put_reg.reshape(70000,28,28)
print_first_10(out_put[-10:])


