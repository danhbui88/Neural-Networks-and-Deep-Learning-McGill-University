# -*- coding: utf-8 -*-
"""Week 8 - Exercise

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19DfBlAC7EvlYDMq4kYxcsuQCh257FsvM

# Deploy Keras Model to the Cloud
1. Download data
2. Hot Encode Categorical Columns
3. Develop Model (use: ```RMSprop(lr=, rho=, epsilon=, decay=)```)
3. Create Storage Bucket (should already exist)
4. Authenticate on Google Cloud Platform
5. Deploy your model using the ```gcloud``` tool

### Download Data
"""

!wget https://storage.googleapis.com/nicksdemobucket/bank.csv

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.train import RMSPropOptimizer
from tensorflow import keras
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
import time

print(__import__('sys').version)
print(tf.__version__)
print(tf.keras.__version__)

import numpy as np
import pandas as pd
df = pd.read_csv('bank.csv', sep=";")
df.head()

df.describe()

"""### Hot Encode Categorical Columns"""

df.loc[df['y'] == 'no', 'y'] = 0
df.loc[df['y'] == 'yes', 'y'] = 1
df.head()

# Get all categorical columns
categorical = df.select_dtypes(include='object').columns
print(categorical)

# Hot encode all categorical columns
for i in categorical:
  new_cols = pd.get_dummies(df[i], prefix=i)
  df = df.drop(i, axis=1)
  df = df.join(new_cols)
  
df.head()

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()

train, test = train_test_split(df, test_size=0.1)

X_train = min_max_scaler.fit_transform(train.drop(['y'], axis=1))
y_train = train[['y']].values

X_test = min_max_scaler.fit_transform(test.drop(['y'], axis=1))
y_test = test[['y']].values

X_train.shape, y_train.shape, X_test.shape, y_test.shape

"""### Develop Model
* Use ```RMSprop(lr=, rho=, epsilon=, decay=)```
* Plot accuracy and loss over time
"""

model=Sequential()
model.add(Dense(8, activation='relu', input_shape=(51,)))
model.add(Dense(1))
model.compile(loss='mean_squared_error',
              optimizer = tf.keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=1e-08, decay=0.00),
              metrics=['mean_absolute_error', 'mean_squared_error'])
model.summary()

early_stopping_monitor=EarlyStopping(monitor='val_mean_squared_error',patience=5)
start = time.time()
h=model.fit(X_train,y_train,epochs=300,batch_size=128,steps_per_epoch=int(100/4),validation_split=0.2,verbose=1,callbacks=[early_stopping_monitor])
end = time.time()
print("Model took %0.2f seconds to train"%(end - start))

h.history.keys()
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

figure(num=None, figsize=(8,6), dpi=80, facecolor='w',edgecolor='k')
plt.plot(h.history['mean_squared_error'],label='Training MSE')
plt.plot(h.history['val_mean_squared_error'],label='Validation MSE')
plt.xlabel('Epochs')
plt.ylabel('Model Error')
plt.legend(loc='best')
plt.show()

"""### Create Storage"""

PROJECT_ID = "automl-first-experience" #@param {type:"string"}

# This must absolutely be a GLOBALLY UNIQUE name
BUCKET_NAME = "minhbui-ml-demo-bucket" #@param {type:"string"}
REGION = "us-central1" #@param {type:"string"}

! gcloud config set project $PROJECT_ID
! echo $PROJECT_ID

"""### Authenticate on Google Cloud Platform
* Click on link and enter secret key
"""

import sys

if 'google.colab' in sys.modules:
  from google.colab import auth as google_auth
  google_auth.authenticate_user()
else:
#   %env GOOGLE_APPLICATION_CREDENTIALS ''

# Create Bucket
!gsutil mb -p $PROJECT_ID -l $REGION gs://$BUCKET_NAME
# Display what is in the bucket
print('Bucket Content:')
!gsutil ls -al gs://$BUCKET_NAME

JOB_DIR = 'gs://' + BUCKET_NAME + '/mcgilldemo'
print(JOB_DIR)

"""### Export Model to Google Cloud Storage"""

export_path = tf.contrib.saved_model.save_keras_model(model, JOB_DIR + '/keras_export')
print("Model exported to: ", export_path)

"""### Deploy Model from GCS"""

MODEL_NAME = "mcgill_week8_first_model"
! gcloud ml-engine models create $MODEL_NAME --regions $REGION
### Create model
MODEL_VERSION = "v1"
# Get a list of directories in the `keras_export` parent directory
KERAS_EXPORT_DIRS = ! gsutil ls $JOB_DIR/keras_export/
# Pick the directory with the latest timestamp
SAVED_MODEL_PATH = KERAS_EXPORT_DIRS[-1]
# Create model version based on that SavedModel directory
! gcloud ml-engine versions create $MODEL_VERSION --model $MODEL_NAME --runtime-version 1.13 --python-version 3.5 --framework tensorflow --origin $SAVED_MODEL_PATH

#Predict an input sample
X_test[0]

%%bash
rm prediction_input.json
touch prediction_input.json 
echo "[0.5135135135135136, 0.021828673763804596, 0.6, 0.09852365170231998, 0.0, \
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, \
0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, \
0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]">> prediction_input.json
cat prediction_input.json

#This command used to predict the input json file above. Note: All the input must be json file format
!gcloud ml-engine predict --model mcgill_week8_first_model --version v1 --json-instances prediction_input.json

#! gcloud ml-engine predict --help

"""### Test your model with your test set"""

# Create file with features to send to model
with open('test.json', 'w') as f:
  for item in X_test[:20]:
    item = list(item)
    f.write("%s\n" % item)
! cat test.json

# Send file to model for prediction

! gcloud ml-engine predict --model $MODEL_NAME --version $MODEL_VERSION --json-instances test.json

# Delete model. Note: Used in case the model is broken. We delete to rerun the coding
! gcloud ml-engine versions delete v1 --quiet --model mcgill_week8_first_model
! gcloud ml-engine models delete mcgill_week8_first_model --quiet
