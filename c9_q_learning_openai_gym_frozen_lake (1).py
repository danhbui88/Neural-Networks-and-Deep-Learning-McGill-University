# -*- coding: utf-8 -*-
"""C9 - Q Learning - OpenAI Gym - Frozen Lake

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ORz7LGbxyYDQoOIkGeXwlWCJl6M89eT
"""

!pip install gym

import gym
import random
import itertools
import matplotlib
import matplotlib.pyplot as plt 
import matplotlib.style
import numpy as np
import pandas as pd
import sys

from collections import defaultdict

matplotlib.style.use('ggplot')

env=gym.make('FrozenLake-v0')

from google.colab import files
src = list(files.upload().values())[0]
open('plotting.py','wb').write(src)
import plotting

#Make the Epsilon-greedy policy
def createEpsilonGreedyPolicy(Q, epsilon, num_actions):
  def policyFunction(state):
    Action_probabilities = np.ones(num_actions, dtype = float) * epsilon/num_actions
    best_action = np.argmax(Q[state])
    Action_probabilities[best_action] += (1.0 - epsilon)
    return Action_probabilities
  return policyFunction

#Build Q-Learning Model
def qLearning(env,num_episodes,discount_factor=1.0,alpha=0.3,epsilon=0.1):
  Q = defaultdict(lambda: np.zeros(env.action_space.n))
  stats= plotting.EpisodeStats(episode_lengths = np.zeros(num_episodes),
                                episode_rewards = np.zeros(num_episodes))
  policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)
  for ith_episode in range(num_episodes):
    state = env.reset()
    for t in itertools.count():
      action_probabilities = policy(state)
      action = np.random.choice(np.arange(len(action_probabilities)),
                                p = action_probabilities)
      next_state, reward, done, _ = env.step(action)
      
      stats.episode_rewards[ith_episode] += reward
      stats.episode_lengths[ith_episode] = t
      
      best_next_action = np.argmax(Q[next_state])
      td_target= reward + discount_factor * Q[next_state][best_next_action]
      td_delta = td_target - Q[state][action]
      Q[state][action] += alpha * td_delta
      
      if done:
        break
      
      state = next_state
  return Q, stats

# Train the model
Q, stats = qLearning(env,2000)

#Plot important statistics
plotting.plot_episode_stats(stats)

env.render()

"""**After many trials, the network was able to reach the highest reward of 1 (the target state). With this Q-Learning method, it doesn't seem getting the result faster compared to Q-Table method. However, this Q-learning method can give us nice visualization about statistics.**"""

