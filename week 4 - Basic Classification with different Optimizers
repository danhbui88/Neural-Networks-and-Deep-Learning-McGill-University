# -*- coding: utf-8 -*-
"""Week 4 Basic Classification Assignment 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/danhbui88/d9e5e5fd25dadd7ea3a5dbbd96f75480/week-4-basic-classification-assignment-4.ipynb
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import Adagrad
import numpy as np
import matplotlib.pyplot as plt

#Getting the data fashion_mnist from keras
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images.shape

#we have 60,000 samples and each of them represented by 28x28 pixels

train_labels

#The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255, since we need the output between 0-1
train_images = train_images / 255.0

test_images = test_images / 255.0

def get_new_model(input_shape=(28,28)):
  model = Sequential()
  model.add(Flatten(input_shape=(28, 28)))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(10, activation='softmax'))
  return(model)

lr_to_test=[1E-7,1E-6,1E-5,1E-4,1E-3,1E-2,1E-1,1E0,1E1,1E2]

Adam_acc=[]
for lr in lr_to_test:
  print('\n\nTesting Adam model with learning rate: %f\n'%lr )
  model1 = get_new_model()
  optimizer1 = Adam(lr=lr)
  model1.compile(optimizer=optimizer1,loss='sparse_categorical_crossentropy',metrics=['accuracy'])
  h1=model1.fit(train_images,train_labels,epochs=5)
  Adam_acc.append(max(h1.history['acc']))

SGD_acc=[]
for lr in lr_to_test:  
  print('\n\nTesting SGD model with learning rate: %f\n'%lr )
  model2=get_new_model()
  optimizer2 = SGD(lr=lr)
  model2.compile(optimizer=optimizer2,loss='sparse_categorical_crossentropy',metrics=['accuracy'])
  h2=model2.fit(train_images,train_labels,epochs=5)
  SGD_acc.append(max(h2.history['acc']))

Adagrad_acc=[]
for lr in lr_to_test:  
  print('\n\nTesting Adagrad model with learning rate: %f\n'%lr )
  model3=get_new_model()
  optimizer3 = Adagrad(lr=lr)
  model3.compile(optimizer=optimizer3,loss='sparse_categorical_crossentropy',metrics=['accuracy'])
  h3=model3.fit(train_images,train_labels,epochs=5)
  Adagrad_acc.append(max(h3.history['acc']))

print(max(Adam_acc))
print(max(SGD_acc))
print(max(Adagrad_acc))

xi = [i for i in range(0, len(lr_to_test))]
plt.plot(xi,Adam_acc,label="Adam",color='r')
plt.plot(xi,SGD_acc,label='SGD',color='b')
plt.plot(xi,Adagrad_acc,label='Adagrad',color='green')
plt.ylim(0,1)
plt.xlabel('Learning Rates')
plt.ylabel('Accuracy') 
plt.xticks(xi, lr_to_test)
plt.title('Accuracy and Learning Rate Comparison between 3 different Optimizers')
plt.legend(loc='best') 
plt.show()
plt.show()

